{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkasolver as ps\n",
    "from pkasolver import util\n",
    "from pkasolver import analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import DataLoader\n",
    "import random\n",
    "\n",
    "#define PairData Class\n",
    "class PairData(Data):\n",
    "    def __init__(self, edge_index, x, edge_index2, x2):\n",
    "        super(PairData, self).__init__()\n",
    "        self.edge_index = edge_index\n",
    "        self.x = x\n",
    "        self.edge_index2 = edge_index2\n",
    "        self.x2 = x2\n",
    "\n",
    "    def __inc__(self, key, value):\n",
    "        if key == 'edge_index':\n",
    "            return self.x.size(0)\n",
    "        if key == 'edge_index2':\n",
    "            return self.x2.size(0)\n",
    "        else:\n",
    "            return super().__inc__(key, value)\n",
    "        \n",
    "# data = PairData(edge_index_p, x_p, edge_index_d, x_d)\n",
    "# data_list = [data, data]\n",
    "# loader = DataLoader(data_list, batch_size=2, follow_batch=['x_p', 'x_d'] )\n",
    "# batch = next(iter(loader))\n",
    "\n",
    "\n",
    "def make_nodes(mol):\n",
    "    x = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        x.append(\n",
    "            np.array(\n",
    "                [\n",
    "                    #atom.GetIdx() + num_atoms * i,\n",
    "                    #float(atom.GetProp(\"_GasteigerCharge\")),\n",
    "                    atom.GetSymbol() == \"C\",\n",
    "                    atom.GetSymbol() == \"O\",\n",
    "                    atom.GetSymbol() == \"N\",\n",
    "                    atom.GetSymbol() == \"P\",\n",
    "                    atom.GetSymbol() == \"F\",\n",
    "                    atom.GetSymbol() == \"Cl\",\n",
    "                    atom.GetSymbol() == \"I\",\n",
    "                    atom.GetFormalCharge(),\n",
    "                    atom.GetChiralTag(),\n",
    "                    atom.GetHybridization(),\n",
    "                    atom.GetNumExplicitHs(),\n",
    "                    atom.GetIsAromatic(),\n",
    "                    atom.GetTotalValence(),\n",
    "                    atom.GetTotalDegree()\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    return torch.tensor(np.array([np.array(xi) for xi in x]), dtype=torch.float)\n",
    "\n",
    "def make_edges_and_attr(mol):\n",
    "    edges = []\n",
    "    edge_attr = []\n",
    "    for bond in mol.GetBonds():\n",
    "        edges.append(\n",
    "            np.array(\n",
    "                [\n",
    "                    [bond.GetBeginAtomIdx()],\n",
    "                    [bond.GetEndAtomIdx()],\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        edge_attr.append(\n",
    "            [bond.GetBondTypeAsDouble(), bond.GetIsConjugated()]\n",
    "        )\n",
    "    edge_index = torch.tensor(np.hstack(np.array(edges)), dtype=torch.long)\n",
    "    edge_attr = torch.tensor(np.array(edge_attr), dtype=torch.float)\n",
    "    return edge_index, edge_attr\n",
    "\n",
    "def Mol_to_PairData(prot, deprot):\n",
    "    x_p = make_nodes(prot)\n",
    "    edge_index_p, edge_attr_p = make_edges_and_attr(prot)\n",
    "    \n",
    "    x_d = make_nodes(deprot)\n",
    "    edge_index_d, edge_attr_d = make_edges_and_attr(deprot)\n",
    "    \n",
    "    data = PairData(edge_index_p, x_p, edge_index_d, x_d)\n",
    "    data.edge_attr = edge_attr_p\n",
    "    data.edge_attr2 = edge_attr_d\n",
    "    return data.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_Bal = \"../data/Baltruschat/\"\n",
    "SDFfile1 = data_folder_Bal + \"combined_training_datasets_unique.sdf\"\n",
    "SDFfile2 = data_folder_Bal + \"novartis_cleaned_mono_unique_notraindata.sdf\"\n",
    "SDFfile3 = data_folder_Bal + \"AvLiLuMoVe_cleaned_mono_unique_notraindata.sdf\"\n",
    "# specify device\n",
    "#device = 'cpu'\n",
    "device = 'cuda'\n",
    "\n",
    "\n",
    "df1 = ps.util.import_sdf(SDFfile1)\n",
    "df2 = ps.util.import_sdf(SDFfile2)\n",
    "df3 = ps.util.import_sdf(SDFfile3)\n",
    "\n",
    "#Data corrections:\n",
    "df1.marvin_atom[90] = \"3\"\n",
    "\n",
    "df1 = util.conjugates_to_DataFrame(df1)\n",
    "df1 = util.sort_conjugates(df1)\n",
    "df1 = util.pka_to_ka(df1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PairData(edge_attr=[24, 2], edge_attr2=[24, 2], edge_index=[2, 24], edge_index2=[2, 24], x=[21, 14], x2=[21, 14], y=[1]) \n",
      "\n",
      " tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 3., 0., 0., 3., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 1., 4., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 1., 4., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 1., 4., 3.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 1., 0., 3., 1., 1., 4., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 3., 0., 1., 4., 3.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 3., 0., 1., 3., 2.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 3., 0., 0., 3., 3.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 4., 4.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 4., 0., 0., 1., 1.]]) \n",
      "\n",
      " tensor([[ 0,  1,  2,  3,  4,  5,  4,  7,  8,  9, 10, 11, 11, 13, 14,  9, 16, 17,\n",
      "         18,  8,  6, 12, 15, 19],\n",
      "        [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
      "         19, 20,  0,  7, 13, 17]])\n"
     ]
    }
   ],
   "source": [
    "#create pyG Dataset\n",
    "\n",
    "dataset = []\n",
    "for i in range(len(df1.index)):\n",
    "    dataset.append(Mol_to_PairData(df1.protonated[i],df1.deprotonated[i]))\n",
    "    dataset[i].y = torch.tensor([float(df1.pKa[i])], dtype=torch.float32, device=device)\n",
    "\n",
    "print(dataset[0], '\\n\\n' ,dataset[0].x,'\\n\\n', dataset[0].edge_index)\n",
    "\n",
    "#split train and test set\n",
    "\n",
    "random.shuffle(dataset)\n",
    "\n",
    "split_length=int(len(dataset)*train_test_split)\n",
    "train_dataset = dataset[:split_length]\n",
    "test_dataset = dataset[split_length:]\n",
    "#create Dataloader objects that contain batches \n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, follow_batch=['x', 'x2'])\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, follow_batch=['x', 'x2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch(batch=[989], edge_attr=[1048, 2], edge_attr2=[1048, 2], edge_index=[2, 1048], edge_index2=[2, 1048], x=[989, 14], x2=[989, 14], x2_batch=[989], x_batch=[989], y=[64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch)\n",
    "dataset[0].num_features\n",
    "#dataset[0].num_edge_features\n",
    "len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set Hyperparameters\n",
    "train_test_split = 0.8\n",
    "hidden_channels = 64\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "num_epochs = 10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): NNConv(14, 96)\n",
      "  (conv2): NNConv(96, 64)\n",
      "  (conv3): NNConv(64, 64)\n",
      "  (conv4): NNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=1, bias=True)\n",
      ") GCN(\n",
      "  (conv1): NNConv(14, 96)\n",
      "  (conv2): NNConv(96, 64)\n",
      "  (conv3): NNConv(64, 64)\n",
      "  (conv4): NNConv(64, 64)\n",
      "  (lin): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Sequential as Seq, Linear as Lin, ReLU\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import NNConv\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_max_pool\n",
    "from torch_geometric.nn import global_add_pool\n",
    "from torch import optim\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(1)\n",
    "        \n",
    "        num_features = dataset[0].num_features\n",
    "        num_edge_features = dataset[0].num_edge_features\n",
    "        \n",
    "        \n",
    "        nn = Seq(Lin(num_edge_features, 16), ReLU(), Lin(16, dataset[0].num_node_features* 96))\n",
    "        self.conv1 = NNConv(dataset[0].num_node_features, 96, nn=nn)\n",
    "        nn = Seq(Lin(num_edge_features, 16), ReLU(), Lin(16, 96* hidden_channels))\n",
    "        self.conv2 = NNConv(96, hidden_channels, nn=nn)\n",
    "        nn = Seq(Lin(num_edge_features, 16), ReLU(), Lin(16, hidden_channels* hidden_channels))\n",
    "        self.conv3 = NNConv(hidden_channels, hidden_channels, nn=nn)\n",
    "        self.conv4 = NNConv(hidden_channels, hidden_channels, nn=nn)\n",
    "        self.lin = Linear(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, batch, edge_attr):\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = x.relu()\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = x.relu() + 0.000001\n",
    "        \n",
    "        return x\n",
    "\n",
    "model_p = GCN(hidden_channels=hidden_channels).to(device=device)\n",
    "model_d = GCN(hidden_channels=hidden_channels).to(device=device)\n",
    "print(model_p, model_d)\n",
    "\n",
    "optimizer1 = torch.optim.Adam(model_p.parameters(), lr=learning_rate)\n",
    "optimizer2 = torch.optim.Adam(model_d.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "criterion_v = torch.nn.L1Loss() # that's the MAE Loss\n",
    "scheduler1 = optim.lr_scheduler.ReduceLROnPlateau(optimizer1, patience=5, verbose=True)\n",
    "scheduler2 = optim.lr_scheduler.ReduceLROnPlateau(optimizer2, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train MAE: 2.0326, Test MAE: 2.0299\n",
      "Epoch: 002, Train MAE: 1.9965, Test MAE: 1.9988\n",
      "Epoch: 003, Train MAE: 1.9708, Test MAE: 1.9817\n",
      "Epoch: 004, Train MAE: 2.0007, Test MAE: 1.9977\n",
      "Epoch: 005, Train MAE: 1.9780, Test MAE: 1.9821\n",
      "Epoch: 006, Train MAE: 1.9974, Test MAE: 1.9993\n",
      "Epoch: 007, Train MAE: 2.3370, Test MAE: 2.3106\n",
      "Epoch: 008, Train MAE: 1.9646, Test MAE: 1.9667\n",
      "Epoch: 009, Train MAE: 1.9980, Test MAE: 1.9961\n",
      "Epoch: 010, Train MAE: 1.9301, Test MAE: 1.9398\n",
      "Epoch: 011, Train MAE: 1.9655, Test MAE: 1.9721\n",
      "Epoch: 012, Train MAE: 1.9236, Test MAE: 1.9711\n",
      "Epoch: 013, Train MAE: 1.9140, Test MAE: 1.9554\n",
      "Epoch: 014, Train MAE: 1.9768, Test MAE: 2.0060\n",
      "Epoch: 015, Train MAE: 1.9664, Test MAE: 1.9772\n",
      "Epoch: 016, Train MAE: 1.9447, Test MAE: 1.9660\n",
      "Epoch: 017, Train MAE: 1.9679, Test MAE: 1.9750\n",
      "Epoch: 018, Train MAE: 1.9744, Test MAE: 2.0065\n",
      "Epoch: 019, Train MAE: 1.9071, Test MAE: 1.9147\n",
      "Epoch: 020, Train MAE: 1.8831, Test MAE: 1.8986\n",
      "Epoch: 021, Train MAE: 1.8861, Test MAE: 1.9131\n",
      "Epoch: 022, Train MAE: 1.9313, Test MAE: 1.9700\n",
      "Epoch: 023, Train MAE: 1.8789, Test MAE: 1.9222\n",
      "Epoch: 024, Train MAE: 1.8575, Test MAE: 1.8838\n",
      "Epoch: 025, Train MAE: 1.8745, Test MAE: 1.8893\n",
      "Epoch: 026, Train MAE: 1.8504, Test MAE: 1.8809\n",
      "Epoch: 027, Train MAE: 1.8579, Test MAE: 1.8865\n",
      "Epoch: 028, Train MAE: 1.8752, Test MAE: 1.8849\n",
      "Epoch: 029, Train MAE: 1.8242, Test MAE: 1.8605\n",
      "Epoch: 030, Train MAE: 1.9297, Test MAE: 1.9904\n",
      "Epoch: 031, Train MAE: 1.8296, Test MAE: 1.8656\n",
      "Epoch: 032, Train MAE: 1.8192, Test MAE: 1.8472\n",
      "Epoch: 033, Train MAE: 1.8909, Test MAE: 1.9120\n",
      "Epoch: 034, Train MAE: 1.8227, Test MAE: 1.8543\n",
      "Epoch: 035, Train MAE: 1.9692, Test MAE: 1.9793\n",
      "Epoch: 036, Train MAE: 1.9463, Test MAE: 1.9555\n",
      "Epoch: 037, Train MAE: 1.9659, Test MAE: 1.9681\n",
      "Epoch: 038, Train MAE: 1.8479, Test MAE: 1.8678\n",
      "Epoch: 039, Train MAE: 1.8153, Test MAE: 1.8562\n",
      "Epoch: 040, Train MAE: 1.9865, Test MAE: 2.0314\n",
      "Epoch: 041, Train MAE: 1.8357, Test MAE: 1.8620\n",
      "Epoch: 042, Train MAE: 1.8052, Test MAE: 1.8442\n",
      "Epoch: 043, Train MAE: 1.7843, Test MAE: 1.8258\n",
      "Epoch: 044, Train MAE: 1.8379, Test MAE: 1.8829\n",
      "Epoch: 045, Train MAE: 1.8239, Test MAE: 1.8503\n",
      "Epoch: 046, Train MAE: 1.7743, Test MAE: 1.8108\n",
      "Epoch: 047, Train MAE: 1.7728, Test MAE: 1.8237\n",
      "Epoch: 048, Train MAE: 1.7781, Test MAE: 1.8447\n",
      "Epoch: 049, Train MAE: 1.7845, Test MAE: 1.8450\n",
      "Epoch: 050, Train MAE: 1.8353, Test MAE: 1.8828\n",
      "Epoch: 051, Train MAE: 1.7775, Test MAE: 1.8099\n",
      "Epoch: 052, Train MAE: 1.8510, Test MAE: 1.9604\n",
      "Epoch: 053, Train MAE: 1.8652, Test MAE: 1.8844\n",
      "Epoch: 054, Train MAE: 3.1114, Test MAE: 3.2206\n",
      "Epoch: 055, Train MAE: 1.7440, Test MAE: 1.7831\n",
      "Epoch: 056, Train MAE: 1.7540, Test MAE: 1.8056\n",
      "Epoch: 057, Train MAE: 1.7114, Test MAE: 1.7672\n",
      "Epoch: 058, Train MAE: 1.6950, Test MAE: 1.7518\n",
      "Epoch: 059, Train MAE: 1.7244, Test MAE: 1.7868\n",
      "Epoch: 060, Train MAE: 1.6903, Test MAE: 1.7654\n",
      "Epoch: 061, Train MAE: 1.7267, Test MAE: 1.8196\n",
      "Epoch: 062, Train MAE: 1.7293, Test MAE: 1.7725\n",
      "Epoch: 063, Train MAE: 1.6631, Test MAE: 1.7381\n",
      "Epoch: 064, Train MAE: 1.6356, Test MAE: 1.7131\n",
      "Epoch: 065, Train MAE: 1.6373, Test MAE: 1.7222\n",
      "Epoch: 066, Train MAE: 1.7491, Test MAE: 1.8118\n",
      "Epoch: 067, Train MAE: 1.6837, Test MAE: 1.7429\n"
     ]
    }
   ],
   "source": [
    "def train(loader):\n",
    "    model_p.train()\n",
    "    model_d.train()\n",
    "    for data in loader:  # Iterate in batches over the training dataset. \n",
    "        prot_out = model_p(data.x, data.edge_index, data.x_batch,  data.edge_attr)  # Perform a single forward pass.\n",
    "        #print(data.x, data.edge_index, data.x_batch,  data.edge_attr) \n",
    "        deprot_out = model_d(data.x2, data.edge_index2, data.x2_batch,  data.edge_attr2)\n",
    "        #out = prot_out \n",
    "        out = torch.log10(torch.div(deprot_out, prot_out))\n",
    "        #print('prot_out',out)\n",
    "        loss = criterion(out.flatten(), data.y)  # Compute the loss.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer1.step()  # Update parameters based on gradients.\n",
    "        optimizer2.step()\n",
    "        optimizer1.zero_grad() # Clear gradients.\n",
    "        optimizer2.zero_grad()\n",
    "        \n",
    "def test(loader):\n",
    "    model_p.eval()\n",
    "    model_d.eval()\n",
    "    loss = torch.Tensor([0]).to(device=device)\n",
    "    for data in loader:  # Iterate in batches over the training dataset.\n",
    "        prot_out = model_p(data.x, data.edge_index, data.x_batch,  data.edge_attr)  # Perform a single forward pass.\n",
    "        deprot_out = model_d(data.x2, data.edge_index2, data.x2_batch,  data.edge_attr2)\n",
    "        #out = prot_out \n",
    "        out = torch.log10(torch.div(deprot_out, prot_out))\n",
    "        loss += criterion_v(out.flatten(), data.y)\n",
    "    return loss/len(loader) # MAE loss of batches can be summed and divided by the number of batches\n",
    "     \n",
    "for epoch in range(1, num_epochs+1):\n",
    "    train(train_loader)\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'Epoch: {epoch:03d}, Train MAE: {train_acc.item():.4f}, Test MAE: {test_acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
